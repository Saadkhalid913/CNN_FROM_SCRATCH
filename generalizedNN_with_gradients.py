# -*- coding: utf-8 -*-
"""GeneralizedNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ADnMdBrxund66-wk9jfHHCSckQYN9nT1
"""



import numpy as np

from sklearn.datasets import make_moons 
# import matplotlib.pyplot as plt 
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split


n_features = 2 
n_samples = 10000
n_classes = 2
lr = 0.01

x,y = make_moons(n_samples)
x_train, x_test, y_train, y_test = train_test_split(x,y)

n_samples = x_train.shape[0]

y_truth = np.zeros((n_samples, 2))

for i in range(n_samples):
  y_truth[i][y_train[i]] = 1

def MSELoss(y_pred, y_truth, derivative = False):
    if derivative:
      return -1 * (y_truth - y_pred)
    return np.power((y_truth - y_pred), 2) / 2

def sigmoid(x, derivative = False):
    if derivative:
      return sigmoid(x) * (1 - sigmoid(x))
    return 1 / (1 + np.exp(-x))

class Dense():
    def __init__(self, input_neurons: int, output_neurons: int, activation):
        self.Weights = np.random.randn(input_neurons, output_neurons)
        self.Biases = np.random.randn(1, output_neurons)
        self.Activation = activation

    def Forward(self, x: np.array):
        assert x.shape[1] == self.Weights.shape[0]
        Z1 = np.dot(x, self.Weights)
        A1 = self.Activation(Z1)

        return Z1, A1


    def Backprop(self, Z1: np.array, A0: np.array, gradient: np.array, learning_rate = 0.001, next_weights=None):
        assert gradient.shape[1] == self.Weights.shape[1]
        OldWeights = self.Weights.copy()
        if not next_weights:
            delta = self.Activation(Z1, derivative = True) *  gradient # (samples, output_weights) 
        else:
            # (samples, output_weights) * (output_weights, next_layer_weights) DOT (samples, output_weights)
            delta = self.Activation(Z1, derivative = True) *  np.dot(next_weights, gradient) 

        WeightUpdate = np.dot(A0.T, delta) 
        BiasUpdate = np.sum(delta, axis = 0, keepdims=True)

        self.Weights += -learning_rate * WeightUpdate
        self.Biases += -learning_rate * BiasUpdate

        return np.dot(delta, OldWeights.T)

D1 = Dense(input_neurons=2, output_neurons=10, activation=sigmoid, learning_rate=lr)
D2 = Dense(input_neurons=10, output_neurons=2, activation=sigmoid, learning_rate=lr)

for i in range(100):
  Z1, A1 = D1.Forward(x_train)
  Z2, A2 = D2.Forward(A1)

  loss = MSELoss(A2,y_truth, derivative=True)

  gradient = D2.Backprop(Z2, A1, loss)
  gradient = D1.Backprop(Z1, x_train, gradient)

Z1, A1 = D1.Forward(x_test)
Z2, A2 = D2.Forward(A1)

print(A2.shape)

print(np.sum(np.argmax(A2, axis = 1) == y_test) / len(y_test))