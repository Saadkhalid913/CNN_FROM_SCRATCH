# -*- coding: utf-8 -*-
"""testNN

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12jgXpF6e_SIpUNE0WILdl4Txwn4k8QLx
"""

import numpy as np
from sklearn.datasets import make_moons 
import matplotlib.pyplot as plt 
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

n_features = 2 
# n_samples = 2000
n_classes = 2
lr = 0.001

x,y = make_moons(n_samples)
x_train, x_test, y_train, y_test = train_test_split(x,y)

n_samples = x_train.shape[0]

y_truth = np.zeros((n_samples,2))
for i in range(n_samples):
  y_truth[i][y_train[i]] = 1

class NN():
  def __init__(self):
    self.W1 = np.random.randn(n_features, 4)
    self.B1 = np.random.randn(1, 4)
    self.W2 = np.random.randn(4, n_classes)
    self.B2 = np.random.randn(1, n_classes) 

  def sigmoid(self, x, derivative = False):
    if derivative:
      return self.sigmoid(x) * (1 - self.sigmoid(x))
    return 1 / (1 + np.exp(-x))

  def forward(self, x):
    Z1 = np.dot(x, self.W1) + self.B1
    A1 = self.sigmoid(Z1)
    Z2 = np.dot(A1, self.W2) + self.B2
    A2 = self.sigmoid(Z2)
    sigmoid_output = self.sigmoid(A2)

    return Z1, A1, Z2, A2

  def predict(self, x):
    Z1 = np.dot(x, self.W1) + self.B1
    A1 = self.sigmoid(Z1)
    Z2 = np.dot(A1, self.W2) + self.B2
    A2 = self.sigmoid(Z2)
    # sigmoid_output = self.sigmoid(A2)

    return A2


  def MSELoss(self, y_pred, y_truth, derivative = False):
    if derivative:
      return 1 * (y_truth - y_pred)
    return np.power((y_truth - y_pred), 2) / 2

  def Backprop(self, n_iters=1000):
    for i in range(n_iters):
      Z1, A1, Z2, A2 = self.forward(x_train)
      oldW2 = self.W2.copy() # we copy these weights for later 
      DLoss = self.MSELoss(y_truth, A2,derivative = True) # Dloss/Dsigmoid output 
      DSigmoid = self.sigmoid(Z2, derivative = True) #Dsigmoid / Dneuron_out_put_sum
      DWeight = A1 # Dneuron_sum/Dweights

      UpdateW2 = np.dot(DWeight.T, DLoss * DSigmoid) # we transpose the weights to perform
                                                     # dot product on the element-wise product 
                                                    # of the loss & the sigmoid derivative
                                                    # this is done because we need to take dot product 
                                                    # of each sample along with the delta (loss * sigmoid)
                                                    # that it creates for each neuron. 
                                                    # this is then summed up (as part of dot product)
                                                    # and becomes the jth value in the ith row of the 
                                                    # delta matrix (4x2) in this case or (hidden_nodes * output_nodes)
                                                    # this is then done for each neuron and its respective weights, resulting
                                                    # in a (4x2) matrix which is the update
      UpdateB2 = np.sum(DLoss * DSigmoid, axis = 0, keepdims=True) / n_samples

      self.W2 += -1 * lr * UpdateW2
      self.B2 += -1 * lr * UpdateB2

      delta3 = np.dot(DLoss * DSigmoid, oldW2.T)

      UpdateB1 = np.sum(delta3 * self.sigmoid(Z1, derivative = True), axis = 0, keepdims=True) 

      UpdateW1 = np.dot(x_train.T, delta3)

      self.W1 += -1 * lr * UpdateW1
      self.B1 += -1 * lr * UpdateB1

for i in range(5):
  n = NN()
  n.Backprop()
  pred = np.argmax(n.predict(x_test), axis = 1)
  print(accuracy_score(pred, y_test))